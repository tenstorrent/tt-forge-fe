name: Model Analysis Test - subroutine

on:
  workflow_call:
    inputs:
      test_mark:
        description: 'Test mark to run'
        required: false
        default: 'not (skip_model_analysis or out_of_memory)'
        type: string
      test_group_cnt:
        description: 'Test group count'
        required: false
        default: "10"
        type: string
      test_group_ids:
        description: 'Test group ids'
        required: false
        default: '[1,2,3,4,5,6,7,8,9,10]'
        type: string
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
      runs-on:
        description: 'Runs on (JSON array of tags, e.g. [\"n150\",\"perf\",\"n300\"]). A literal \"perf\" token expands to [\"n150-perf\",\"llmbox\"].'
        required: false
        type: string
        default: '["runner"]'
      run_id:
        description: 'Run id the workflow where to find installation (or else it will search)'
        required: false
        type: string
      tests_to_filter:
        description: 'Filter specific tests'
        required: false
        type: string
      allow-fail:
        description: 'Allow tests to fail (unstable tests)'
        required: false
        default: false
        type: boolean
      sh-runner:
        description: 'Run tests using shared runners'
        required: false
        type: boolean
      max-job-duration-minutes:
        description: 'Set maximum execution time for the job in minutes. If the job exceeds this limit it will be cancelled and marked failed'
        required: false
        type: number
        default: 360

jobs:

  prepare-runners:
    runs-on: ubuntu-latest
    outputs:
      selected_builds: ${{ steps.set.outputs.selected_builds }}
    steps:
      - name: Decide builds
        id: set
        shell: bash
        run: |
          set -euo pipefail
          INPUT_JSON=${{ inputs.runs-on}}
          echo "Input runs-on JSON: $INPUT_JSON"

          if [[ "$INPUT_JSON" == *'"perf"'* ]]; then
            # Replace "perf" with "n150-perf","llmbox" inside the JSON string
            out_json=$(printf '%s' "$INPUT_JSON" | sed -E 's/"perf"/"n150-perf","llmbox"/g')
          else
            out_json="$INPUT_JSON"
          fi

          # Trim leading/trailing whitespace
          out_json="$(printf '%s' "$out_json" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')"
          
          # Final check to ensure it's not empty, e.g., if input was an empty string
          if [ -z "$out_json" ]; then
            out_json='["runner"]' # Fallback to a safe default JSON array
          fi

          echo "Prepared builds JSON: $out_json"
          # The key is to assign the string directly. Using `printf %q` can help escape, but
          # we want the raw JSON string here.
          echo "selected_builds=$out_json" >> "$GITHUB_OUTPUT"

  run-tests:
    needs: prepare-runners
    timeout-minutes: ${{ inputs.max-job-duration-minutes }}
    strategy:
      fail-fast: false
      matrix:
        build: ${{ fromJson(needs.prepare-runners.outputs.selected_builds) }}
        test_group_id: ${{ fromJSON(inputs.test_group_ids) }}

    # Runner selection rules:
    # - shared -> tt-ubuntu-2204-{tag}-stable
    # - perf runners (n150-perf or llmbox) -> just the tag
    # - otherwise -> ["tag","in-service"]
    runs-on: ${{ inputs.sh-runner && format('tt-ubuntu-2204-{0}-stable', matrix.build) || (matrix.build == 'n150-perf' || matrix.build == 'llmbox') && matrix.build || fromJson(format('["{0}", "in-service"]', matrix.build)) }}

    continue-on-error: ${{ inputs.allow-fail }}

    container:
      image: ${{ inputs.sh-runner && format('harbor.ci.tenstorrent.net/{0}', inputs.docker-image) || inputs.docker-image }}
      options: --device /dev/tenstorrent/0
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache

    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_HOME: /mnt/dockercache/huggingface
      IRD_LF_CACHE: ${{ vars.IRD_LF_CACHE }}
      FORGE_MODELS_CACHE: /mnt/dockercache/forge_models_cache
      HF_HUB_DISABLE_PROGRESS_BARS: 1
      FORGE_DISABLE_REPORTIFY_DUMP: 1
      FORGE_EXTRACT_TVM_UNIQUE_OPS_CONFIG: 1
      FORGE_EXPORT_TVM_UNIQUE_OPS_CONFIG_DETAILS: 1

    name: "run-tests ${{ inputs.test_mark }} (${{ inputs.sh-runner && format('{0}-shared', matrix.build) || matrix.build }}, ${{ matrix.test_group_id }})"

    steps:

      - name: Fetch job id
        id: fetch-job-id
        uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
        with:
          job_name: "run-tests ${{ inputs.test_mark }} (${{ inputs.sh-runner && format('{0}-shared', matrix.build) || matrix.build }}, ${{ matrix.test_group_id }})"

      - name: Set reusable strings
        id: strings
        shell: bash
        env:
          JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
        run: |
          echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
          echo "build-output-dir=$(pwd)/build" >> "$GITHUB_OUTPUT"
          echo "test_report_path=reports/report_$JOB_ID.xml" >> "$GITHUB_OUTPUT"
          echo "runner-temp=$(pwd)/wheels" >> "$GITHUB_OUTPUT"

      - name: Git safe dir
        run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

      - uses: actions/checkout@v4
        with:
          sparse-checkout: |
            .github/workflows/
            env/
            forge/test
            pytest.ini
            conftest.py
            .test_durations
            scripts/

      - name: Setup Forge Models repo
        shell: bash
        run: |
          git submodule update --init --recursive -f third_party/tt_forge_models

      - name: Cleanup submodules
        run: |
          git submodule foreach --recursive git clean -ffdx
          git submodule foreach --recursive git reset --hard

      - name: Ensure artifact temp dir exists
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "${{ steps.strings.outputs.runner-temp }}"
          ls -ld "${{ steps.strings.outputs.runner-temp }}" || true
          echo "Created artifact temp dir: ${{ steps.strings.outputs.runner-temp }}"

      - name: Download wheel
        if: ${{ inputs.run_id }}
        continue-on-error: true
        uses: tenstorrent/tt-forge/.github/actions/download-artifact@main
        with:
          name: forge-wheel
          run_id: ${{ inputs.run_id }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          path: ${{ steps.strings.outputs.runner-temp }}

      - name: Find and download forge wheel
        if: ${{ !inputs.run_id }}
        uses: dawidd6/action-download-artifact@v9
        with:
          github_token: ${{secrets.GITHUB_TOKEN}}
          workflow_conclusion: success
          workflow_search: true
          workflow: on-push.yml
          name: forge-wheel
          repo: tenstorrent/tt-forge-fe
          check_artifacts: true
          search_artifacts: true
          path: ${{ steps.strings.outputs.runner-temp }}

      - name: Install wheel
        shell: bash
        run: |
          source env/activate
          ART_DIR="${{ steps.strings.outputs.runner-temp }}"
          echo "Installing wheels from: $ART_DIR"
          ls -al "$ART_DIR" || true

          # Expand matching wheels into arrays; fail if none found
          tvm_wheels=( "$ART_DIR"/tt_tvm*.whl )
          forge_wheels=( "$ART_DIR"/tt_forge_fe*.whl )

          if [[ ${#tvm_wheels[@]} -eq 0 ]]; then
            echo "ERROR: No tt_tvm wheel found in $ART_DIR"
            exit 1
          fi

          pip install "${tvm_wheels[@]}" --force-reinstall
          if [[ ${#forge_wheels[@]} -ne 0 ]]; then
            pip install "${forge_wheels[@]}" --force-reinstall || true
          else
            echo "No tt_forge_fe wheel found; continuing."
          fi

      - name: Extract and Export Unique Ops Configuration
        shell: bash
        run: |
          set -o pipefail
          source env/activate
          echo "Collecting tests for group ${{ matrix.test_group_id }} with mark '${{ inputs.test_mark }}'..."
          set +e
          pytest_args=(
            "--splits" "${{ inputs.test_group_cnt }}"
            "--group" "${{ matrix.test_group_id }}"
            "--splitting-algorithm" "least_duration"
            "-m" "${{ inputs.test_mark }}"
            "--collect-only"
            "-q"
          )
          if [ -n "${{ inputs.tests_to_filter }}" ]; then
            # Split on commas and trim whitespace
            IFS=',' read -r -a test_filters <<< "${{ inputs.tests_to_filter }}"
            pytest_args+=("--tests_to_filter")
            for tf in "${test_filters[@]}"; do
              pytest_args+=("$(echo "$tf" | xargs)")
            done
          fi
          pytest forge/test/models/ "${pytest_args[@]}" \
                | sed -n '/^Collected tests /,/^collected /p' | sed '/^[Cc]ollected /d' >.pytest_tests_to_run

          if [ $? -ne 0 ]; then
            echo "Failed to collect tests. Doing dry run..."
            set -e
            pytest forge/test/models/ "${pytest_args[@]}"
            exit 1
          fi
          echo "Collected tests."
          cat .pytest_tests_to_run

          python .github/workflows/test_runner.py  \
               --continue-after-crash \
               --junit-xml=${{ steps.strings.outputs.test_report_path }} \
               --log-memory-usage \
               --runxfail \
               --no-skips \
               -v \
            2>&1 | tee extract-and-export-unique-ops-configs.log
          exit_code=${PIPESTATUS[0]}
          exit $exit_code

      - name: Upload Extract And Export Unique Ops Configs Log
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
           name: extract-and-export-unique-ops-configs-log-${{ matrix.build }}-${{ matrix.test_group_id }}-${{ inputs.test_mark }}-${{ steps.fetch-job-id.outputs.job_id }}
           path: extract-and-export-unique-ops-configs.log

      - name: Upload Memory Usage Log
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: memory-usage-${{ matrix.build }}-${{ matrix.test_group_id }}-${{ inputs.test_mark }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: pytest-memory-usage.csv

      - name: Upload Models Unique Ops Config
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: models-unique-ops-config-${{ steps.fetch-job-id.outputs.job_id }}
          path: generated_modules/