name: Test

on:
  workflow_dispatch:
    inputs:
      rebuild:
        description: 'Rebuild the Forge'
        required: false
        default: false
        type: boolean
      preset:
        description: 'Test preset to run'
        required: false
        default: 'Custom'
        type: choice
        options:
          - Full Model Passing
          - Full Model XFailing
          - Models Ops
          - Sweeps
          - Custom
      test_mark:
        description: 'Test mark to run (custom preset)'
        required: false
        default: 'push'
        type: string
      runs-on:
        description: 'Runs on'
        required: false
        type: choice
        options:
          - n150
          - n300
      test_group_cnt:
        description: 'Test group count'
        required: false
        default: "2"
        type: choice
        options:
          - "1"
          - "2"
          - "3"
          - "4"
      operators:
        description: 'Operators to test (comma separated)'
        required: false
        type: string
  workflow_call:
    inputs:
      test_mark:
        description: 'Test mark to run'
        required: false
        default: 'push'
        type: string
      test_group_cnt:
        description: 'Test group count'
        required: false
        default: 2
        type: number
      test_group_ids:
        description: 'Test group ids'
        required: false
        default: '[1,2]'
        type: string
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
      runs-on:
        description: 'Runs on'
        required: false
        type: string
        default: '[{"runs-on": "n150"}, {"runs-on": "n300"}]'
      operators:
        description: 'Operators to test (comma separated)'
        required: false
        type: string
      on_nightly_run_id:
        description: 'Run ID for the on-nightly workflow used to generate the WHL installer'
        required: false
        type: string

permissions:
  packages: write
  checks: write
  pull-requests: write # only required if `comment: true` was enabled

jobs:

  docker-build:
    if: ${{ !inputs.docker-image }}
    uses: ./.github/workflows/build-image.yml
    secrets: inherit

  set-docker-image:
    runs-on: ubuntu-latest
    needs: docker-build
    if: always()
    outputs:
      docker-image: ${{ steps.set-docker.outputs.docker-image }}
      buildtype: ${{ steps.set-docker.outputs.buildtype }}
      test_group_cnt: ${{ steps.set-inputs.outputs.test_group_cnt }}
      test_group_ids: ${{ steps.set-inputs.outputs.test_group_ids }}
      test_mark: ${{ steps.set-inputs.outputs.test_mark }}
      runs-on: ${{ steps.set-inputs.outputs.runs-on }}
      operators: ${{ steps.set-inputs.outputs.operators }}
    steps:
      - name: Set docker image
        id: set-docker
        run: |
          if [ -z "${{ inputs.docker-image }}" ]; then
            echo "docker-image=${{ needs.docker-build.outputs.docker-image }}" >> $GITHUB_OUTPUT
          else
            echo "docker-image=${{ inputs.docker-image }}" >> $GITHUB_OUTPUT
          fi
          if [ ${{ inputs.rebuild }} == 'true' ]; then
            echo "buildtype=Release" >> $GITHUB_OUTPUT
          else
            echo "buildtype=None" >> $GITHUB_OUTPUT
          fi
      - name: Inputs Management
        id: set-inputs
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && -z "${{ github.event.inputs }}" ]]; then
            echo "test_mark=${{ inputs.test_mark }}" >> $GITHUB_OUTPUT
            echo "test_group_cnt=${{ inputs.test_group_cnt }}" >> $GITHUB_OUTPUT
            echo "test_group_ids=${{ inputs.test_group_ids }}" >> $GITHUB_OUTPUT
            echo "runs-on=${{ inputs.runs-on }}" >> $GITHUB_OUTPUT
            echo "operators=${{ inputs.operators }}" >> $GITHUB_OUTPUT
          else
            echo "test_group_cnt=${{ inputs.test_group_cnt }}" >> $GITHUB_OUTPUT
            echo "test_group_ids=[$(seq -s ',' 1 ${{ inputs.test_group_cnt }})]" >> $GITHUB_OUTPUT
            echo "runs-on=[{\"runs-on\": \"${{ inputs.runs-on }}\"}]" >> $GITHUB_OUTPUT
            echo "operators=${{ inputs.operators }}" >> $GITHUB_OUTPUT
            case "${{ inputs.preset }}" in
              "Full Model Passing")
                echo "test_mark='nightly and not xfail'" >> $GITHUB_OUTPUT
                ;;
              "Full Model XFailing")
                echo "test_mark='nightly and xfail'" >> $GITHUB_OUTPUT
                ;;
              "Models Ops")
                echo "test_mark='nightly_models_ops'" >> $GITHUB_OUTPUT
                ;;
              "Sweeps")
                echo "test_mark='nightly_sweeps'" >> $GITHUB_OUTPUT
                ;;
              "Custom")
                echo "test_mark=${{ inputs.test_mark }}" >> $GITHUB_OUTPUT
                ;;
              *)
                echo "Invalid preset"
                exit 1
                ;;
            esac
          fi

  build:
    needs: set-docker-image
    uses: ./.github/workflows/build.yml
    secrets: inherit
    with:
      docker-image: ${{ needs.set-docker-image.outputs.docker-image }}
      build: ${{ needs.set-docker-image.outputs.buildtype}}


  run-tests:
    needs:
      - set-docker-image
      - build

    strategy:
      fail-fast: false
      matrix:
        build: ${{ needs.set-docker-image.outputs.runs-on }}
        test_group_id: ${{ needs.set-docker-image.outputs.test_group_ids }}

    runs-on:
      - in-service
      - ${{ matrix.build.runs-on }}

    container:
      image: ${{ needs.set-docker-image.outputs.docker-image }}
      options: --device /dev/tenstorrent/0
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache

    name: "run-tests ${{ needs.set-docker-image.outputs.test_mark }} (${{ matrix.build.runs-on }}, ${{ matrix.test_group_id }})"
    steps:

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "run-tests ${{ needs.set-docker-image.outputs.test_mark }} (${{ matrix.build.runs-on }}, ${{ matrix.test_group_id }})"

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "build-output-dir=$(pwd)/build" >> "$GITHUB_OUTPUT"
        echo "test_report_path=reports/report_$JOB_ID.xml" >> "$GITHUB_OUTPUT"

    - name: Git safe dir
      run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

    - uses: actions/checkout@v4
      with:
          sparse-checkout: |
            env/
            forge/test
            pytest.ini
            conftest.py
            .test_durations

    # Clean everything from submodules (needed to avoid issues
    # with cmake generated files leftover from previous builds)
    - name: Cleanup submodules
      run: |
          git submodule foreach --recursive git clean -ffdx
          git submodule foreach --recursive git reset --hard

    - name: Download wheel
      if: ${{ inputs.rebuild == null || inputs.rebuild == 'true' }}
      uses: actions/download-artifact@v4
      with:
        name: forge-wheel
        # Prioritize input run ID, if not available use current run (for new builds)
        run-id: ${{ inputs.on_nightly_run_id || github.run_id }}
        github-token: ${{ github.token }}

    - name: Find and download forge wheel
      if: ${{ inputs.rebuild == 'false' }}
      uses: dawidd6/action-download-artifact@v6
      with:
        github_token: ${{secrets.GITHUB_TOKEN}}
        workflow_conclusion: success
        workflow: on-nightly.yml
        branch: master
        name: forge-wheel
        repo: tenstorrent/tt-forge-fe
        check_artifacts: true

    - name: Install wheel
      shell: bash
      run: |
        source env/activate
        pip install tvm*.whl --force-reinstall
        pip install forge*.whl --force-reinstall

    - name: Run Test
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        HF_HOME: /mnt/dockercache/huggingface
        FORGE_MODELS_CACHE: /mnt/dockercache/forge_models_cache
        HF_HUB_DISABLE_PROGRESS_BARS: 1
        FORGE_DISABLE_REPORTIFY_DUMP: 1
        OPERATORS: ${{ needs.set-docker-image.outputs.operators }}
      shell: bash
      run: |
        source env/activate
        pytest --splits ${{ needs.set-docker-image.outputs.test_group_cnt }} \
               --group ${{ matrix.test_group_id }} \
               --splitting-algorithm least_duration \
               -m "${{ needs.set-docker-image.outputs.test_mark }}" \
               --junit-xml=${{ steps.strings.outputs.test_report_path }}

    - name: Upload Test Report
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: test-reports-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ steps.fetch-job-id.outputs.job_id }}
        path: ${{ steps.strings.outputs.test_report_path }}

    - name: Show Test Report
      uses: mikepenz/action-junit-report@v5
      if: success() || failure()
      with:
        report_paths: ${{ steps.strings.outputs.test_report_path }}
        check_name: TT-Forge-FE Tests
        comment: false
        updateComment: false
        detailed_summary: true
        group_suite: true
