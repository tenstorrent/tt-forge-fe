name: Build and Test

on:
  workflow_dispatch:
  workflow_call:
  push:

jobs:

  build-image:
    runs-on: builder
    outputs:
      docker-image: ${{ steps.build.outputs.docker-image }}
    steps:
      - name: Fix permissions
        shell: bash
        run: sudo chown ubuntu:ubuntu -R $(pwd)

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build Docker images and output the image name
        id: build
        shell: bash
        run: |
          # Output the image name
          set pipefail
          .github/build-docker-images.sh | tee docker.log
          DOCKER_CI_IMAGE=$(tail -n 1 docker.log)
          echo "DOCKER_CI_IMAGE $DOCKER_CI_IMAGE"
          echo "docker-image=$DOCKER_CI_IMAGE" >> "$GITHUB_OUTPUT"

  build-and-test:

    needs: build-image
    strategy:
      fail-fast: false
      matrix:
        build:
          - runs-on: runner
        test_group_id: [1]

    runs-on:
      - in-service
      - ${{ matrix.build.runs-on }}

    container:
      image: ${{ needs.build-image.outputs.docker-image }}
      options: --device /dev/tenstorrent/0
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
    steps:

    - name: Set reusable strings
      id: strings
      shell: bash
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "build-output-dir=$(pwd)/build" >> "$GITHUB_OUTPUT"

        # Github job context unfortunately doesn't contain job_id, this is the workaround how to fetch it using GH API
        JOB_ID=$(curl -s -H "Authorization: token ${{ github.token }}" \
          "https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/jobs" | \
          jq -r '.jobs[] | select(.name | contains("${{ github.job }}")) | .id')
        echo "Current job id: $JOB_ID"

        echo "job-id=$JOB_ID" >> "$GITHUB_OUTPUT"
        echo "test_report_path=reports/report_$JOB_ID.xml" >> "$GITHUB_OUTPUT"

    - name: Git safe dir
      run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

    - uses: actions/checkout@v4
      with:
          submodules: recursive
          fetch-depth: 0 # Fetch all history and tags

    # Clean everything from submodules (needed to avoid issues
    # with cmake generated files leftover from previous builds)
    - name: Cleanup submodules
      run: |
          git submodule foreach --recursive git clean -ffdx
          git submodule foreach --recursive git reset --hard

    - name: ccache
      uses: hendrikmuhs/ccache-action@v1.2
      with:
        create-symlink: true
        key: ${{ matrix.build.runs-on }}-runtime-${{ matrix.build.enable_runtime }}-${{ env.SDK_VERSION }}

    - name: Build
      shell: bash
      run: |
        source env/activate
        cmake -G Ninja \
        -B ${{ steps.strings.outputs.build-output-dir }} \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_C_COMPILER=clang \
        -DCMAKE_CXX_COMPILER=clang++ \
        -DCMAKE_C_COMPILER_LAUNCHER=ccache \
        -DCMAKE_CXX_COMPILER_LAUNCHER=ccache
        cmake --build ${{ steps.strings.outputs.build-output-dir }}

    - name: Run Unit Tests
      shell: bash
      run: |
        source env/activate
        cmake --build ${{ steps.strings.outputs.build-output-dir }} -- run_unit_tests

    - name: Run Test
      shell: bash
      run: |
        source env/activate
        set -o pipefail # Ensures that the exit code reflects the first command that fails
        pip install pytest-split
        # pytest --splits 2 --group ${{ matrix.test_group_id }} --junit-xml=${{ steps.strings.outputs.test_report_path }} 2>&1 | tee pytest.log
        pytest --store-durations --junit-xml=${{ steps.strings.outputs.test_report_path }} 2>&1 | tee pytest.log

    - name: Upload Test Durations
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: test-durations-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}
        path: .test_durations

    - name: Upload Test Log
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: test-log-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}
        path: pytest.log

    - name: Run Perf Benchmark
      shell: bash
      run: |
        source env/activate
        python forge/test/benchmark/benchmark.py -m mnist_linear -bs 1 -o forge-benchmark-e2e-mnist.json

    - name: Upload Test Report
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: test-reports-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}
        path: ${{ steps.strings.outputs.test_report_path }}

    - name: Show Test Report
      uses: mikepenz/action-junit-report@v4
      if: success() || failure()
      with:
        report_paths: ${{ steps.strings.outputs.test_report_path }}
        check_name: TT-Forge-FE Tests
