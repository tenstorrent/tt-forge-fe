name: Perf benchmark

on:
  workflow_call:
    inputs:
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string

jobs:

  run-perf-benchmarks:

    strategy:
      fail-fast: false
    runs-on:
      - in-service
      - n150
      - performance

    container:
      image: ${{ inputs.docker-image }}
      options: --device /dev/tenstorrent/0
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
    steps:

    - name: Fetch job id
      id: fetch-job-id
      uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
      with:
        job_name: "${{ github.job }}"

    - name: Set reusable strings
      id: strings
      shell: bash
      env:
        JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
      run: |
        echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
        echo "build-output-dir=$(pwd)/build" >> "$GITHUB_OUTPUT"
        echo "perf_report_path=forge-benchmark-e2e-mnist_$JOB_ID.json" >> "$GITHUB_OUTPUT"

    - name: Git safe dir
      run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

    - uses: actions/checkout@v4
      with:
          sparse-checkout: |
            env/
            forge/test
            pytest.ini
            conftest.py
            .test_durations
          fetch-depth: 0 # Fetch all history and tags

    # Clean everything from submodules (needed to avoid issues
    # with cmake generated files leftover from previous builds)
    - name: Cleanup submodules
      run: |
          git submodule foreach --recursive git clean -ffdx
          git submodule foreach --recursive git reset --hard

    - name: Download build artifacts
      uses: actions/download-artifact@v4
      with:
        name: forge-wheel

    - name: Install wheel
      shell: bash
      run: |
        source env/activate
        pip install tvm*.whl --force-reinstall
        pip install forge*.whl --force-reinstall

    - name: Run Perf Benchmark
      shell: bash
      run: |
        source env/activate
        python forge/test/benchmark/benchmark.py -m mnist_linear -bs 1 -lp 32 -o ${{ steps.strings.outputs.perf_report_path }}

    - name: Upload Perf Report
      uses: actions/upload-artifact@v4
      if: success() || failure()
      with:
        name: perf-reports
        path: ${{ steps.strings.outputs.perf_report_path }}
