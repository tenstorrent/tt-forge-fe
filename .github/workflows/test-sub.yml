name: Test - subroutine

on:
  workflow_call:
    inputs:
      test_mark:
        description: 'Test mark to run'
        required: false
        default: 'push'
        type: string
      test_group_cnt:
        description: 'Test group count'
        required: false
        default: "2"
        type: string
      test_group_ids:
        description: 'Test group ids'
        required: false
        default: '[1,2]'
        type: string
      docker-image:
        description: 'Docker image to use for build'
        required: true
        type: string
      runs-on:
        description: 'Runs on'
        required: false
        type: string
        default: '[{"runs-on": "n150"}, {"runs-on": "n300"}, {"runs-on": "p150"}]'
      operators:
        description: 'Operators to test (comma separated)'
        required: false
        type: string
      filters:
        description: 'Filters for tests (comma separated)'
        required: false
        type: string
      run_id:
        description: 'Run id the workflow where to find installation (or else it will search)'
        required: false
        type: string
      allow-fail:
        description: 'Allow tests to fail (unstable tests)'
        required: false
        default: false
        type: boolean
      continue-on-crash:
        description: 'Continue if test crashes, otherwise restart the test'
        required: false
        type: boolean
      sh-runner:
        description: 'Run tests using shared runners'
        required: false
        type: boolean

jobs:
  run-tests:

    strategy:
      fail-fast: false
      matrix:
        build: ${{ fromJson(inputs.runs-on) }}
        test_group_id: ${{ fromJSON(inputs.test_group_ids) }}

    runs-on: ${{ inputs.sh-runner && format('tt-beta-ubuntu-2204-{0}-large-stable', matrix.build.runs-on) || fromJson(format('["{0}", "in-service"]', matrix.build.runs-on)) }}

    continue-on-error: ${{ inputs.allow-fail }}

    container:
      image: ${{ inputs.sh-runner && format('harbor.ci.tenstorrent.net/{0}', inputs.docker-image) || inputs.docker-image }}
      options: --device /dev/tenstorrent/0
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env
        - /mnt/dockercache:/mnt/dockercache

    name: "run-tests ${{ inputs.test_mark }} (${{ inputs.sh-runner && format('{0}-shared', matrix.build.runs-on) || (matrix.build.runs-on) }}, ${{ matrix.test_group_id }})"

    steps:

      - name: Fetch job id
        id: fetch-job-id
        uses: tenstorrent/tt-github-actions/.github/actions/job_id@main
        with:
          job_name: "run-tests ${{ inputs.test_mark }} (${{ inputs.sh-runner && format('{0}-shared', matrix.build.runs-on) || (matrix.build.runs-on) }}, ${{ matrix.test_group_id }})"

      - name: Set reusable strings
        id: strings
        shell: bash
        env:
          JOB_ID: ${{ steps.fetch-job-id.outputs.job_id }}
        run: |
          echo "work-dir=$(pwd)" >> "$GITHUB_OUTPUT"
          echo "build-output-dir=$(pwd)/build" >> "$GITHUB_OUTPUT"
          echo "test_report_path=reports/report_$JOB_ID.xml" >> "$GITHUB_OUTPUT"

      - name: Git safe dir
        run: git config --global --add safe.directory ${{ steps.strings.outputs.work-dir }}

      - uses: actions/checkout@v4
        with:
          sparse-checkout: |
            .github/workflows/
            env/
            forge/test
            pytest.ini
            conftest.py
            .test_durations

      - name: Setup Forge Models repo
        shell: bash
        run: |
          git submodule update --init --recursive -f third_party/tt_forge_models

        # Clean everything from submodules (needed to avoid issues
        # with cmake generated files leftover from previous builds)
      - name: Cleanup submodules
        run: |
          git submodule foreach --recursive git clean -ffdx
          git submodule foreach --recursive git reset --hard

      - name: Create temporary directory for artifacts
        run: mkdir -p ${{ runner.temp }}/artifacts/

      - name: Download wheel
        if: ${{ inputs.run_id }}
        continue-on-error: true
        uses: tenstorrent/tt-forge/.github/actions/download-artifact@main
        with:
          name: forge-wheel
          run_id: ${{ inputs.run_id }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          path: ${{ runner.temp }}/artifacts/

      - name: Find and download forge wheel
        if: ${{ !inputs.run_id }}
        uses: dawidd6/action-download-artifact@v9
        with:
          github_token: ${{secrets.GITHUB_TOKEN}}
          workflow_conclusion: success
          workflow_search: true
          workflow: on-push.yml
          name: forge-wheel
          repo: tenstorrent/tt-forge-fe
          check_artifacts: true
          search_artifacts: true
          path: ${{ runner.temp }}/artifacts/

      - name: Install wheel
        shell: bash
        run: |
          source env/activate
          pip install tt_tvm*.whl --upgrade
          pip install tt_forge_fe*.whl --upgrade

      - name: Run Sanity Tests
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_HOME: /mnt/dockercache/huggingface
          FORGE_MODELS_CACHE: /mnt/dockercache/forge_models_cache
          HF_HUB_DISABLE_PROGRESS_BARS: 1
          FORGE_DISABLE_REPORTIFY_DUMP: 1
        shell: bash
        run: |
          source env/activate
          echo "Collecting slim wheel tests..."
          pytest --splits 1 \
               --group 1 \
               --splitting-algorithm least_duration \
               -m "slim_wheel" --collect-only -q forge/test/test_slim_wheel.py \
                | sed -n '/^Collected tests /,/^collected /p' | sed '/^[Cc]ollected /d' >.pytest_slim_wheel_tests_to_run

          echo "Collected slim wheel tests."
          cat .pytest_slim_wheel_tests_to_run

          set +e
          python .github/workflows/test_runner.py  \
               --junit-xml=${{ steps.strings.outputs.test_report_path }}_slim_wheel \
               --log-memory-usage \
               $(if [ "${{inputs.continue-on-crash}}" = "true" ]; then echo "--continue-after-crash"; fi)
          slim_wheel_exit_code=$?

          if [ -f "crashed_pytest.log" ]; then
            echo "Creating crash summary for slim wheel tests..."
            echo "### Crashed slim wheel tests:" >>$GITHUB_STEP_SUMMARY
            cat crashed_pytest.log >>$GITHUB_STEP_SUMMARY
            mv crashed_pytest.log crashed_pytest_slim_wheel.log
          fi

          if [ $slim_wheel_exit_code -ne 0 ]; then
            echo "Slim wheel tests failed with exit code $slim_wheel_exit_code"
            exit $slim_wheel_exit_code
          fi

      - name: Upload Slim Wheel Test Log
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: slim_wheel-test-log-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ inputs.test_mark }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: pytest.log

      - name: Upload Slim Wheel Test Crash Log
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: slim_wheel-test-crash-log-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ inputs.test_mark }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: crashed_pytest_slim_wheel.log

      - name: Upload Slim Wheel Test Report
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: slim_wheel-test-reports-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: ${{ steps.strings.outputs.test_report_path }}_slim_wheel

      - name: Show Slim Wheel Test Report
        uses: mikepenz/action-junit-report@v5
        if: success() || failure()
        with:
          report_paths: ${{ steps.strings.outputs.test_report_path }}_slim_wheel
          check_name: TT-Forge-FE Slim Wheel Tests
          comment: false
          updateComment: false
          detailed_summary: true
          group_suite: true

      - name: Install dev requirements
        shell: bash
        run: |
            source env/activate
            pip install -r env/dev_requirements.txt

      - name: Run Test
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_HOME: /mnt/dockercache/huggingface
          FORGE_MODELS_CACHE: /mnt/dockercache/forge_models_cache
          HF_HUB_DISABLE_PROGRESS_BARS: 1
          FORGE_DISABLE_REPORTIFY_DUMP: 1
          OPERATORS: ${{ inputs.operators }}
          FILTERS: ${{ inputs.filters }}
        shell: bash
        run: |
          source env/activate
          echo "Collecting tests for group ${{ matrix.test_group_id }} with mark '${{ inputs.test_mark }}'..."
          set +e
          pytest --splits ${{ inputs.test_group_cnt }} \
               --group ${{ matrix.test_group_id }} \
               --splitting-algorithm least_duration \
               -m "${{ inputs.test_mark }}" --collect-only -q \
                | sed -n '/^Collected tests /,/^collected /p' | sed '/^[Cc]ollected /d' >.pytest_tests_to_run

          if [ $? -ne 0 ]; then
            echo "Failed to collect tests. Doing dry run..."
            set -e
            pytest --splits ${{ inputs.test_group_cnt }} \
                --group ${{ matrix.test_group_id }} \
                --splitting-algorithm least_duration \
                -m "${{ inputs.test_mark }}" --collect-only -svv
            exit 1
          fi
          echo "Collected tests."
          # enable next line for debugging
          # cat .pytest_tests_to_run

          python .github/workflows/test_runner.py  \
               --junit-xml=${{ steps.strings.outputs.test_report_path }} \
               --log-memory-usage \
               $(if [ "${{inputs.continue-on-crash}}" = "true" ]; then echo "--continue-after-crash"; fi)
          exit_code=$?

          if [ -f "crashed_pytest.log" ]; then
            echo "Creating crash summary for the job..."
            echo "### Crashed tests:" >>$GITHUB_STEP_SUMMARY
            cat crashed_pytest.log >>$GITHUB_STEP_SUMMARY
          fi
          exit $exit_code

      - name: Upload Test Log
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: test-log-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ inputs.test_mark }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: pytest.log

      - name: Upload Test Crash Log
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: test-crash-log-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ inputs.test_mark }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: crashed_pytest.log

      - name: Upload Memory Usage Log
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: memory-usage-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ inputs.test_mark }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: pytest-memory-usage.csv

      - name: Upload Test Report
        uses: actions/upload-artifact@v4
        if: success() || failure()
        with:
          name: test-reports-${{ matrix.build.runs-on }}-${{ matrix.test_group_id }}-${{ steps.fetch-job-id.outputs.job_id }}
          path: ${{ steps.strings.outputs.test_report_path }}

      - name: Show Test Report
        uses: mikepenz/action-junit-report@v5
        if: success() || failure()
        with:
          report_paths: ${{ steps.strings.outputs.test_report_path }}
          check_name: TT-Forge-FE Tests
          comment: false
          updateComment: false
          detailed_summary: true
          group_suite: true
