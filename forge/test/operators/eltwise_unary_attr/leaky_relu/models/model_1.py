# SPDX-FileCopyrightText: Â© 2024 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0
#
#   Test 1 
#   LeakyRelu operators defined by Forge API
#   These kinds of tests test only single specific operator through different Forge architectures
# 


import torch
from torch.distributions import Normal

import forge
import forge.op
import forge.op.nn as nn

from forge import ForgeModule, Tensor


class ForgeLeakyReluTest(ForgeModule):
    """
        Forge Test 1

    """

    INPUTS_RANGE_MIN = -1.0
    INPUTS_RANGE_MAX = 1.0
    INPUTS_DISTRIBUTION = Normal

    WEIGHTS_RANGE_MIN = -1.0
    WEIGHTS_RANGE_MAX = 1.0
    WEIGHTS_DISTRIBUTION = Normal

    def __init__(
        self,
        shape,
        alpha
    ):
        super().__init__("Forge Test 1")

        self.testname = "Operator LeakyRelu, Test 1"
        self.shape = shape
        self.alpha = alpha
        
        self.train_param = forge.Parameter(*self.shape, requires_grad=True)

        input = ForgeLeakyReluTest.INPUTS_DISTRIBUTION(
            ForgeLeakyReluTest.INPUTS_RANGE_MIN, 
            ForgeLeakyReluTest.INPUTS_RANGE_MAX).sample(self.shape)
        self.inputs = [Tensor.create_from_torch(input)]

        weights = ForgeLeakyReluTest.WEIGHTS_DISTRIBUTION(
            ForgeLeakyReluTest.WEIGHTS_RANGE_MIN, 
            ForgeLeakyReluTest.WEIGHTS_RANGE_MAX).sample(self.shape)
        weights.requires_grad = True
        self.set_parameter("train_param", weights)

    def forward(self, x):

        # Layer 2
        mul = forge.op.Multiply("mul", x, self.train_param)

        # Layer 3
        lrelu = forge.op.LeakyRelu("lrelu", mul, alpha=self.alpha)

        return lrelu
