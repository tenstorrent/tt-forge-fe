{   
    "with_pytorch" : true,
    "with_forge" : true,
    "load_dataset_from_disk" : true,
    "prefiltered_dataset_dir" : "/proj_sw/large-model-cache/falcon7b/datasets",
    "sequence_length" : 128,
    "lora_modules" : ["wk", "wq", "wv" ,"dense", "dense_h_to_4h" ,"dense_4h_to_h"],
    "rank" : 2,
    "optimizer_on_host" : true,
    "ignore_pad_token_loss" : true,
    "explicit_pad_token" : true,
    "version" : "padded_split",
    "precision" : "low-mp",
    "forge_device" : "silicon",
    "batch_size" : 4,
    "num_accumulation_steps": 1,
    "num_lora_layers" : 1,
    "num_layers" : 1,
    "num_samples" : 8,
    "num_epochs" : 2,
    "wandb_log_steps" : 20,
    "placement_overwrites" : true,
    "placement_overwrites_seqlen" : false,
    "filter_long_seq" : false,
    "max_grad_norm": 0.3,
    "learning_rate": 0.0002,
    "dataset_name": "guanaco_en_sp_fr",
    "ci": true,
    "no_wandb": true
}
