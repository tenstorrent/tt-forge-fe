# SPDX-FileCopyrightText: Â© 2024 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

# Built-in modules
import pytest
import time
import socket
import subprocess
import json
import random
import os
from datetime import datetime

# Third-party modules
import torch
from torch import nn
from transformers import ResNetForImageClassification
from datasets import load_dataset

# Forge modules
import forge
from forge.verify.compare import compare_with_golden
from test.utils import download_model

# Common constants
GIT_REPO_NAME = "tenstorrent/tt-forge-fe"
REPORTS_DIR = "./benchmark_reports/"

# Batch size configurations
BATCH_SIZE = [
    8,
]

# Input size configurations
INPUT_SIZE = [
    (224, 224),
]

# Channel size configurations
CHANNEL_SIZE = [
    3,
]

# Loop count configurations
LOOP_COUNT = [1, 2, 4, 8, 16, 32]

variants = [
    "microsoft/resnet-50",
]


@pytest.mark.parametrize("variant", variants, ids=variants)
@pytest.mark.parametrize("channel_size", CHANNEL_SIZE, ids=[f"channel_size={item}" for item in CHANNEL_SIZE])
@pytest.mark.parametrize("input_size", INPUT_SIZE, ids=[f"input_size={item}" for item in INPUT_SIZE])
@pytest.mark.parametrize("batch_size", BATCH_SIZE, ids=[f"batch_size={item}" for item in BATCH_SIZE])
@pytest.mark.parametrize("loop_count", LOOP_COUNT, ids=[f"loop_count={item}" for item in LOOP_COUNT])
def test_resnet_hf(
    training,
    batch_size,
    input_size,
    channel_size,
    loop_count,
    variant,
):

    if training:
        pytest.skip("Training is not supported")

    # TODO: This we will need when when we run resnet with real data.
    # Load tiny dataset
    # dataset = load_dataset("zh-plus/tiny-imagenet")
    # images = random.sample(dataset["valid"]["image"], 10)

    # Random data
    input_sample = [torch.rand(batch_size, channel_size, *input_size, dtype=torch.bfloat16)]
    print(f"{input_sample}")

    # Load framework model
    framework_model = download_model(
        ResNetForImageClassification.from_pretrained, variant, return_dict=False, torch_dtype=torch.bfloat16
    )
    # for param in framework_model.parameters():
    #     new_param = param.to(torch.bfloat16)
    #     param.data = new_param.data

    # framework_model = framework_model.to(dtype=torch.bfloat16)
    fw_out = framework_model(*input_sample)

    # Compile model
    compiler_cfg = forge.config.CompilerConfig()
    compiler_cfg.default_df_override = forge.DataFormat.Float16_b
    compiled_model = forge.compile(framework_model, *input_sample, compiler_cfg=compiler_cfg)
    # Run for the first time to warm up the model.
    # This is required to get accurate performance numbers.
    co_out = compiled_model(*input_sample)
    start = time.time()
    for _ in range(loop_count):
        co_out = compiled_model(*input_sample)
    end = time.time()

    co_out = [co.to("cpu") for co in co_out]
    assert [compare_with_golden(golden=fo, calculated=co, pcc=0.95) for fo, co in zip(fw_out, co_out)]

    short_hash = subprocess.check_output(["git", "rev-parse", "--short", "HEAD"]).decode("ascii").strip()
    date = datetime.now().strftime("%d-%m-%Y")
    machine_name = socket.gethostname()
    total_time = end - start
    total_samples = batch_size * loop_count

    samples_per_sec = total_samples / total_time
    model_name = "Resnet 50 HF"
    model_type = "Classification, Random Input Data"
    dataset_name = "Resnet, Random Data"
    num_layers = 50  # Number of layers in the model, in this case 50 layers

    print("====================================================================")
    print("| Resnet Benchmark Results:                                        |")
    print("--------------------------------------------------------------------")
    print(f"| Model: {model_name}")
    print(f"| Model type: {model_type}")
    print(f"| Dataset name: {dataset_name}")
    print(f"| Date: {date}")
    print(f"| Machine name: {machine_name}")
    print(f"| Total execution time: : {total_time}")
    print(f"| Total samples: {total_samples}")
    print(f"| Sample per second: {samples_per_sec}")
    print(f"| Batch size: {batch_size}")
    print(f"| Input size: {input_size}")
    print("====================================================================")

    result = {
        "model": model_name,
        "model_type": model_type,
        "run_type": f"{'_'.join(model_name.split())}_{batch_size}_{'_'.join([str(dim) for dim in input_size])}_{num_layers}_{loop_count}",
        "config": {"model_size": "small"},
        "num_layers": num_layers,
        "batch_size": batch_size,
        "precision": "f32",  # This is we call dataformat, it should be generic, too, but for this test we don't experiment with it
        # "math_fidelity": math_fidelity, @TODO - For now, we are skipping these parameters, because we are not supporting them
        "dataset_name": dataset_name,
        "profile_name": "",
        "input_sequence_length": -1,  # When this value is negative, it means it is not applicable
        "output_sequence_length": -1,  # When this value is negative, it means it is not applicable
        "image_dimension": f"{input_size[0]}x{input_size[1]}",
        "perf_analysis": False,
        "training": training,
        "measurements": [
            {
                "iteration": 1,  # This is the number of iterations, we are running only one iteration.
                "step_name": model_name,
                "step_warm_up_num_iterations": 0,
                "measurement_name": "total_samples",
                "value": total_samples,
                "target": -1,  # This value is negative, because we don't have a target value.
                "device_power": -1.0,  # This value is negative, because we don't have a device power value.
                "device_temperature": -1.0,  # This value is negative, because we don't have a device temperature value.
            },
            {
                "iteration": 1,  # This is the number of iterations, we are running only one iteration.
                "step_name": model_name,
                "step_warm_up_num_iterations": 0,
                "measurement_name": "total_time",
                "value": total_time,
                "target": -1,  # This value is negative, because we don't have a target value.
                "device_power": -1.0,  # This value is negative, because we don't have a device power value.
                "device_temperature": -1.0,  # This value is negative, because we don't have a device temperature value.
            },
        ],
        "device_info": {
            "device_name": "",
            "galaxy": False,
            "arch": "",
            "chips": 1,
        },
        "device_ip": None,
    }

    return result


def resnet_hf_benchmark(config: dict):

    training = config["training"]
    batch_size = config["batch_size"]
    input_size = INPUT_SIZE[0]
    channel_size = CHANNEL_SIZE[0]
    output_file = config["output"]
    loop_count = config["loop_count"]
    variant = variants[0]

    result = test_resnet_hf(
        training=training,
        batch_size=batch_size,
        input_size=input_size,
        channel_size=channel_size,
        loop_count=loop_count,
        variant=variant,
    )

    if not os.path.exists(REPORTS_DIR):
        os.makedirs(REPORTS_DIR)
    if not output_file:
        output_file = f"forge-benchmark-e2e-resnet50_{result['run_type']}.json"
    result["output"] = REPORTS_DIR + output_file

    # Save the results to a file
    with open(result["output"], "w") as f:
        json.dump(result, f)
