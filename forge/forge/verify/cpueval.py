# SPDX-FileCopyrightText: Â© 2024 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0
"""
Golden verification on CPU
"""

from typing import Tuple, Dict, List

from loguru import logger
import torch
import tensorflow as tf

from ..tensor import to_pt_tensors
from ..forgeglobal import get_devices
from ..utils import detach_tensors
import forge
from forge.tvm_utils import map_tf_dtype_to_pt, map_pt_dtype_to_tf


def cpueval_inference(
    inputs: List[Tuple[torch.Tensor, ...]], parameters: List[Dict[str, torch.Tensor]], sequential: bool
) -> List[Tuple[torch.Tensor, ...]]:
    """
    Use CPU/Pytorch to run inference of the full pipeline of devices, equivalen to what run_inference would do.
    This uses the initial graph for forge models, or pytorch for pytorch models.

    Parameters
    ----------
    inputs: List[Tuple[torch.Tensor, ...]]
        Full batch of ordered inputs into the first device

    parameters: List[Dict[str, torch.Tensor]]
        Parameters, for each device in pipeline

    Returns
    -------
    List[Tuple[Tensor, ...]]
        Output of the last device in pipeline, one tuple per input
    """
    devices = get_devices()
    assert len(devices) == len(parameters), "Mismatched number of devices and parameters"

    from forge.run.impl import _run_command
    from forge.run.commands import Command

    ret = []
    for input in inputs:
        data = to_pt_tensors(input)
        for d, params in zip(devices, parameters):
            data = _run_command(
                d, sequential, Command.cpueval_forward(data, params, save_for_backward=False, targets=[]), response=True
            )["result"]
        ret.append(data)

    return ret


class TrainingEvalData:
    """
    Structure holding all golden data calculate from runnig training eval, to be compared with
    data running on Tenstorrent device / model for verification
    """

    class GradData:
        inputs: List[torch.Tensor]  # Ordered input gradients
        parameters: Dict[str, torch.Tensor]  # Gradients for each parameter

        def __init__(self, inputs: List[torch.Tensor], parameters: Dict[str, torch.Tensor] = {}):
            self.inputs = inputs
            self.parameters = parameters

    class DeviceData:
        grad: List["TrainingEvalData.GradData"]  # Gradients, one for each input
        final_parameters: Dict[str, torch.Tensor]  # final parameters, after step of training

        def __init__(self):
            self.grad = []
            self.final_parameters = {}

    devices: List["TrainingEvalData.DeviceData"]  # One set of data for each device in pipeline


def cpueval_training(
    inputs: List[Tuple[torch.Tensor, ...]],
    parameters: List[Dict[str, torch.Tensor]],
    targets: List[Tuple[torch.Tensor, ...]],
    sequential: bool,
    scale_loss: float,
    lr=None,
) -> TrainingEvalData:
    """
    Use CPU/Pytorch to run training on the full pipeline of devices, similar to what run_training would do.
    Inputs should contain a full batch of inputs, and optimizer will run once at the end. Return values include
    all input gradients (if any), gradients for each parameter, both for each input, as well as the final
    updated weights after optimizer.

    Parameters
    ----------
    inputs: List[Tuple[Tensor, ...]]
        A full batch of ordered inputs into the first device

    parameters: List[Dict[str, Tensor]]
        Initial values for all parameters in the model, one per device. This needs to be a separate copy
        of parameter tensors, to keep it separate from the real run which will adjust parameters through training.

    targets: List[Tuple[Tensor, ...]]
        Training targets for each input

    Returns
    -------
    TrainingEvalData
    """
    devices = get_devices()

    from forge.run.impl import _run_command
    from forge.run.commands import Command

    ret = TrainingEvalData()
    ret.devices = []
    for d in devices:
        ret.devices.append(TrainingEvalData.DeviceData())

    for i, d in enumerate(devices):
        optim = d.get_pytorch_optimizer(parameters[i], lr=lr)  # Reinitialize learning rate if given
        if optim:
            optim.zero_grad()

    for input, target in zip(inputs, targets):
        data = to_pt_tensors(
            input, convert_format=False
        )  # TODO: convert for silicon runs to get more accurate comparison
        fw_outputs = []
        for i, d in enumerate(devices):
            logger.trace("Running forward on {}", d)
            detached_data = detach_tensors(data)
            device_targets = target if (d == devices[-1]) else []
            data = _run_command(
                d,
                sequential,
                Command.cpueval_forward(detached_data, parameters[i], save_for_backward=True, targets=device_targets),
                response=True,
            )["result"]
            logger.trace("Forward out on {} is {}", d, data)
            fw_outputs.append(data)

        bw_input_grads = []
        for i in reversed(range(len(devices))):
            d = devices[i]
            logger.trace("Running backward on {}", d)
            grads = _run_command(d, sequential, Command.cpueval_backward(bw_input_grads, parameters[i]), response=True)
            bw_input_grads = grads["input_grads"]
            bw_parameter_grads = grads["params_grads"]
            ret.devices[i].grad.append(TrainingEvalData.GradData(inputs=bw_input_grads, parameters=bw_parameter_grads))

    for i, d in enumerate(devices):
        if isinstance(d, forge.cpudevice.CPUDevice) and d.framework == "tensorflow":
            assert all([x.name in parameters[i] for x in d.modules[0].module.trainable_variables])

            cpu_grads = [
                tf.Variable(grad.detach().numpy(), dtype=map_pt_dtype_to_tf(grad.dtype))
                for grad in ret.devices[i].grad[0].parameters.values()
            ]
            params = [
                tf.Variable(param.detach().numpy(), dtype=map_pt_dtype_to_tf(param.dtype), name=name.split(":")[0])
                for name, param in parameters[i].items()
            ]
            d.optimizer.apply_gradients(zip(cpu_grads, params))

            parameters[i] = {param.name: torch.Tensor(param.numpy()) for param in params}
        else:
            optim = d.get_pytorch_optimizer(parameters[i])
            if optim:
                optim.step()

        ret.devices[i].final_parameters = parameters[i]

    return ret
